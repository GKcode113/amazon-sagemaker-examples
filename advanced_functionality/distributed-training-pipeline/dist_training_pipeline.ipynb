{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook defines a distributed training pipeline **blueprint** that can be used to pre-process the training data, and compile and train the model using any machine learning framework. The use of this blueprint is illustrated through examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AWS CloudFormation Stack\n",
    "\n",
    "This notebook must be opened in an [Amazon SageMaker Notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) created via the  [AWS CloudFormation stack](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacks.html) template: [cfn-sgaemaker-notebook.yaml](./cfn-sagemaker-notebook.yaml). \n",
    "\n",
    "When you use the [template](./cfn-sagemaker-notebook.yaml) to create the CloudFormation stack, you will need to specify the name of an existing [Amazon S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html) in the `S3BucketName` template parameter, and the path to an exisitng [S3 folder](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html#create-folder) in that bucket in the `FSxS3ImportPrefix` template parameter.  The `S3BucketName` S3 bucket must be located in the AWS region of your CloudFormation stack. Default value for `FSxS3ImportPrefix` parameter is `sagemaker`, and it is highly recommended you use this default value, and ensure `sagemaker` exists as a top-level folder in your `S3BucketName` S3 bucket.\n",
    "\n",
    "If you have not already created the CloudFormation stack, close this noteobok, [create the CloudFormation stack using AWS management console](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stack.html), and open this notebook in the SageMaker Notebook instance created via the CloudFormation stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "So that we may not run out of disk space while building Docker container images later in this notebook, configure a new Docker overlay file-system by executing following steps.\n",
    "\n",
    "#### Create new Docker overlay file-system directory:\n",
    "\n",
    "    sudo mkdir -p /home/ec2-user/SageMaker/docker-overlay\n",
    "    sudo chown root:root /home/ec2-user/SageMaker/docker-overlay\n",
    "\n",
    "#### Update the Docker overlay file-system configuration \n",
    "\n",
    "Add following content to `/etc/docker/daemon.json`\n",
    "\n",
    "    {\n",
    "      \"data-root\": \"/home/ec2-user/SageMaker/docker-overlay\",\n",
    "      \"runtimes\": {\n",
    "          \"nvidia\": {\n",
    "              \"path\": \"nvidia-container-runtime\",\n",
    "              \"runtimeArgs\": []\n",
    "          }\n",
    "      }\n",
    "    }\n",
    "\n",
    "#### Restart Docker daemon\n",
    "\n",
    "    sudo systemctl stop docker\n",
    "    sudo systemctl start docker\n",
    "    sudo systemctl status docker.service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Session\n",
    "\n",
    "Let us specify the `s3_bucket` and `s3_prefix` that we will use throughout the notebook. The `s3_bucket` and `s3_prefix`  must be the same as the `S3BucketName` and `FSxS3ImportPrefix` CloudFormation stack parameters, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install sagemaker\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "s3_bucket  =  None # must be same as CloudFormation parameter S3BucketName\n",
    "s3_prefix = 'sagemaker' # must be same as CloudFormation parameter FSxS3ImportPrefix\n",
    "\n",
    "role = get_execution_role() # you may provide a pre-existing role ARN here\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "aws_region = session.region_name\n",
    "print(f\"AWS Region: {aws_region}\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "try:\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.get_bucket_location(Bucket=s3_bucket)\n",
    "    bucket_region = response['LocationConstraint']\n",
    "    bucket_region = 'us-east-1' if bucket_region is None else bucket_region\n",
    "    \n",
    "    print(f\"Bucket region: {bucket_region}\")\n",
    "    \n",
    "    s3_client.head_object(Bucket=s3_bucket, Key=f\"{s3_prefix}/\")\n",
    "    print(f\"Using S3 folder: s3://{s3_bucket}/{s3_prefix}/ in this notebook\")\n",
    "except:\n",
    "    print(f\"Access Error: Check if '{s3_bucket}' S3 bucket is in '{aws_region}' region, and {s3_prefix} path exists\")\n",
    "\n",
    "sts = boto3.client(\"sts\")\n",
    "aws_account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(f\"AWS Account Id: {aws_account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Attached EFS File-system\n",
    "\n",
    "When you create this SageMaker Notebook instance via the [cfn-sagemaker-notebook.yaml](./cfn-sagemaker-notebook.yaml) CloudFormation script, an EFS file-system is automatically created and attached to this SageMaker Notebook instance. Below, we discover the EFS file-system attached to this SageMaker Notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "notebook_attached_efs=!df -kh | grep 'fs-' | sed 's/\\(fs-[0-9a-z]*\\)\\.efs\\..*/\\1/'\n",
    "\n",
    "efs_enabled = False\n",
    "if notebook_attached_efs and re.match(r'fs-[0-9a-z]+', notebook_attached_efs[0]):\n",
    "    efs_enabled=True\n",
    "    print(f\"SageMaker notebook has attached EFS: {notebook_attached_efs}\")\n",
    "else:\n",
    "    print(\"No EFS file-system is attached to this notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover Attached FSx for Lustre File-system\n",
    "\n",
    "When you create this SageMaker Notebook instance via the [cfn-sagemaker-notebook.yaml](./cfn-sagemaker-notebook.yaml) CloudFormation script, a FSx for Lustre file-system is automatically created and attached to this SageMaker Notebook instance, and `s3://S3BucketName/FSxS3ImportPrefix` is automatically imported to the FSx for Lustre file-system. Below, we discover the FSx for Lustre file-system attached to this SageMaker Notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def fsx_file_systems(fsx_client):\n",
    "    \"\"\"Generator for listing Fsx file systems\"\"\"\n",
    "\n",
    "    next_token = None\n",
    "    while True:\n",
    "        if next_token:\n",
    "            resp = fsx_client.describe_file_systems(NextToken=next_token)\n",
    "        else:\n",
    "            resp = fsx_client.describe_file_systems()\n",
    "            \n",
    "        file_systems = resp['FileSystems']\n",
    "        for fs in file_systems:\n",
    "            yield fs\n",
    "\n",
    "        try:\n",
    "            next_token = resp['NextToken']\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "fsx_file_system_id = None\n",
    "\n",
    "notebook_attached_fsx = !df -kh | grep '@tcp:/' \\\n",
    "    | sed 's/\\([0-9a-zA-Z\\.]*\\)@tcp:\\/\\([a-zA-Z0-9]*\\).*/\\1 \\2/'\n",
    "fsx_mount_name = notebook_attached_fsx[0].split()[1]\n",
    "\n",
    "fsx_client = boto3.client(\"fsx\")\n",
    "\n",
    "for fsx_fs in fsx_file_systems(fsx_client):\n",
    "    mount_name = fsx_fs['LustreConfiguration']['MountName']\n",
    "    fs_id = fsx_fs['FileSystemId']\n",
    "    if mount_name == fsx_mount_name:\n",
    "        fsx_file_system_id = fs_id\n",
    "        break\n",
    "        \n",
    "if fsx_file_system_id:\n",
    "    print(f\"FSx for Lustre file-system is attached: {fsx_file_system_id}\")\n",
    "else:\n",
    "    print(f\"No FSx for Lustre file-system is attached\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker File-system  Channels\n",
    "\n",
    "Next define SagerMaker data channels for FSx for Lustre and EFS file-systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Amazon FSx Lustre Data Channel \n",
    "\n",
    "Next, we define the *fsx* data channel using FSx Lustre file-system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "data_channels = None\n",
    "\n",
    "if fsx_file_system_id:\n",
    "    file_system_type = 'FSxLustre'\n",
    "    file_system_access_mode = 'rw'\n",
    "    \n",
    "    # file_system_directory_path below must match the FSxS3ImportPrefix parameter value\n",
    "    # in the CloudFormation stack you used to create this notebook server. Default value is: 'sagemaker'\n",
    "    file_system_directory_path = \"sagemaker\"\n",
    "    \n",
    "    fsx = FileSystemInput(file_system_id=fsx_file_system_id,\n",
    "                           file_system_type=file_system_type,\n",
    "                           directory_path=f\"/{fsx_mount_name}/{file_system_directory_path}\",\n",
    "                           file_system_access_mode=file_system_access_mode)\n",
    "\n",
    "    data_channels = {'fsx': fsx}\n",
    "    print(data_channels)\n",
    "else:\n",
    "    print(\"Fatal Error: FSx for Lustre file-system is not available.\")\n",
    "    print(\"Did you create the SageMaker Notebook via the CloudFormation stack?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Amazon EFS Data Channel \n",
    "\n",
    "Next, we define the *efs* data channel using EFS file-system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "if efs_enabled:\n",
    "    # Specify EFS file system id.\n",
    "    efs_file_system_id = notebook_attached_efs[0]\n",
    "    print(f\"EFS file-system-id: {efs_file_system_id}\")\n",
    "\n",
    "    file_system_access_mode = \"rw\"\n",
    "\n",
    "    # Specify your file system type\n",
    "    file_system_type = \"EFS\"\n",
    "\n",
    "    efs = FileSystemInput(\n",
    "        file_system_id=efs_file_system_id,\n",
    "        file_system_type=file_system_type,\n",
    "        directory_path=f\"/\",\n",
    "        file_system_access_mode=file_system_access_mode,\n",
    "    )\n",
    "    \n",
    "    data_channels['efs'] = efs\n",
    "    print(data_channels)\n",
    "else:\n",
    "    print(\"Fatal Error: EFS file-system is not available.\")\n",
    "    print(\"Did you create the SageMaker Notebook via the CloudFormation stack?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Security Group and Subnet\n",
    "\n",
    "Here, we automatically select a VPC security group and subnet created via the CloudFormation stack. We will use these to run our processing and training jobs, so they have access to the EFS and Fsx for Lustre file-systems available via the VPC subnet.\n",
    "\n",
    "**Note:** For maximum performance, by default, we choose the single subnet used by the FSx for Lustre file-system. However, if that subnet does not have the Amazon EC2 instance capacity for you to launch your processing or training jobs, you may edit the `subnets` variable below to specify your subnet list. To see all available subnets, see `Subnets` under  `Outputs` tab of your CloudFormation stack. For using `trn1` instances, you must specify only one subnet in the `subnets` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "security_group_ids=None\n",
    "subnets=None\n",
    "\n",
    "if fsx_file_system_id:\n",
    "    fsx_client = boto3.client(\"fsx\")\n",
    "    ec2_client = boto3.client('ec2')\n",
    "    \n",
    "    response = fsx_client.describe_file_systems(FileSystemIds=[fsx_file_system_id])\n",
    "    file_system=response['FileSystems'][0]\n",
    "    subnets = file_system['SubnetIds']\n",
    "    network_interface_ids = file_system['NetworkInterfaceIds']\n",
    "         \n",
    "    response = ec2_client.describe_network_interfaces(\n",
    "        NetworkInterfaceIds=network_interface_ids)\n",
    "    network_interface = response['NetworkInterfaces'][0]\n",
    "    groups = network_interface['Groups']\n",
    "    security_group_ids = [ x['GroupId'] for x in groups ]\n",
    "   \n",
    "subnets = list(set(subnets)) if isinstance(subnets, list) else None\n",
    "security_group_ids = list(set(security_group_ids)) if isinstance(security_group_ids, list) \\\n",
    "                        else None\n",
    "\n",
    "print(f\"Subnets: {subnets}\")\n",
    "print(f\"Security groups: {security_group_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Pipeline Spec\n",
    "\n",
    "We specify a *pipeline spec* file that we wish to build and execute in this notebook. Each pipeline spec has a `name`, a `version`, relative path to its `container` directory, and includes a list of pipeline `steps`. Each step in the pipeline has a `name`, a `description`, and a `config` file that defines the step. \n",
    "\n",
    "For example, we specify [neuronx_nemo_megatron/llama2_7b/pipeline.yaml](./examples/neuronx_nemo_megatron/llama2_7b/pipeline.yaml) pipeline, below. Complete the [prerequisites](./examples/neuronx_nemo_megatron/llama2_7b/README.md) for this pipeline before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "\n",
    "pipeline_file=\"examples/neuronx_nemo_megatron/llama2_7b/pipeline.yaml\"\n",
    "with open(pipeline_file, \"r\") as pf:\n",
    "    pipeline_spec=yaml.safe_load(pf)\n",
    "\n",
    "print(json.dumps(pipeline_spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe Build and Push Pipeline Container Image to ECR\n",
    "\n",
    "Next, if `pipeline.container` is specified, we build and push the pipeline spec container image to Amazon ECR. This may take several minutes on first-time build on this notebook. The build log file is created in pipeline spec container path.\n",
    "\n",
    "**Note:** Either `pipeline.image` or `pipeline.container` must be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import sys, os, subprocess, stat\n",
    "\n",
    "container_spec =  pipeline_spec['pipeline'].get('container', None)\n",
    "if container_spec:\n",
    "    container_path = os.path.join(\"containers\", pipeline_spec['pipeline']['container'])\n",
    "    with open(os.path.join(container_path, \"build.log\"), \"w\") as logfile:\n",
    "            print(f\"Building and pushing {container_path} to ECR; see log file: {container_path}/build.log\")\n",
    "            container_build_script = os.path.join(container_path, \"build_tools\", \"build_and_push.sh\")\n",
    "            \n",
    "            st = os.stat(container_build_script)\n",
    "            os.chmod(container_build_script, st.st_mode | stat.S_IEXEC)\n",
    "            subprocess.check_call([container_build_script, aws_region], stdout=logfile, stderr=subprocess.STDOUT)\n",
    "            \n",
    "            image_tag = !cat {container_path}/build_tools/set_env.sh \\\n",
    "                | grep 'IMAGE_TAG' | sed 's/.*IMAGE_TAG=\\(.*\\)/\\1/'\n",
    "            \n",
    "            image_name = !cat {container_path}/build_tools/set_env.sh \\\n",
    "                | grep 'IMAGE_NAME' | sed 's/.*IMAGE_NAME=\\(.*\\)/\\1/'\n",
    "            \n",
    "            ecr_image_uri=f\"{aws_account_id}.dkr.ecr.{aws_region}.amazonaws.com/{image_name[0]}:{image_tag[0]}\"\n",
    "else:\n",
    "    ecr_image_uri = pipeline_spec['pipeline'].get('image', None)\n",
    "\n",
    "assert ecr_image_uri, \"No 'container' or 'image' specified in pipeline spec\"\n",
    "print(ecr_image_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe Download HuggingFace Model Snapshot\n",
    "\n",
    "If `huggingface` object is specified in the pipeline spec, the HuggingFace model snapshot with revision `huggingface.revision` is downloaded and uploaded to the `s3_bucket` under the `s3_prefix`. If `huggingface.tensors` is not set, or set to `false`, model tensors are **not** downloaded from model snaphshot.\n",
    "\n",
    "**Note: You must specify `token` below if the `huggingface.model` requires a HuggingFace token for downloading from the HuggingFace hub.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "hf_spec = pipeline_spec.get('huggingface', None)\n",
    "hf_model_id = None\n",
    "hf_model_revision = None\n",
    "\n",
    "if hf_spec:\n",
    "    token = None # Specify HuggingFace token, if required by model\n",
    "   \n",
    "    hf_model_id = hf_spec.get('model', None)\n",
    "    assert hf_model_id, \"'huggingface.model' is required'\"\n",
    "\n",
    "    hf_model_revision = hf_spec.get('revision', None)\n",
    "    assert hf_model_revision, \"'huggingface.revision' is required'\"\n",
    "\n",
    "    tensors = hf_spec.get('tensors', None)\n",
    "    if not tensors:\n",
    "        print(f\"Downloading {hf_model_id}:{hf_model_revision} from HuggingFace without tensors\")\n",
    "    else:\n",
    "        print(f\"Downloading {hf_model_id}:{hf_model_revision} from HuggingFace Hub with tensors\")\n",
    "   \n",
    "    s3_model_prefix = f\"{s3_prefix}/huggingface/models/{hf_model_id}/{hf_model_revision}\"  # folder where model checkpoint will go\n",
    "    print(f\"s3_model_prefix: {s3_model_prefix}\")\n",
    "\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=s3_bucket, Key=f\"{s3_model_prefix}/config.json\")\n",
    "        print(f\"Skipping download; HuggingFace model already exists at s3://{s3_bucket}/{s3_model_prefix}/\")\n",
    "    except:\n",
    "        subprocess.check_output(f\"pip install huggingface-hub\", shell=True, stderr=subprocess.STDOUT)\n",
    "        from huggingface_hub import snapshot_download\n",
    "        from tempfile import TemporaryDirectory\n",
    "        from pathlib import Path\n",
    "\n",
    "        print(f\"Downloading HuggingFace model snapshot: {hf_model_id}, revision: {hf_model_revision}\")\n",
    "        with TemporaryDirectory(suffix=\"model\", prefix=\"hf\", dir=\".\") as cache_dir:\n",
    "            ignore_patterns = [\"*.msgpack\", \"*.h5\"] if tensors else [ \"*.msgpack\", \"*.h5\", \"*.bin\", \"*.safetensors\"]\n",
    "            snapshot_download(repo_id=hf_model_id, \n",
    "                revision=hf_model_revision, \n",
    "                cache_dir=cache_dir,\n",
    "                ignore_patterns=ignore_patterns,\n",
    "                token=token)\n",
    "\n",
    "            local_model_path = Path(cache_dir)\n",
    "            model_snapshot_path = str(list(local_model_path.glob(f\"**/snapshots/{hf_model_revision}\"))[0])\n",
    "            print(f\"model_snapshot_path: {model_snapshot_path}\")\n",
    "\n",
    "            print(\"Uploading snapshot to S3...\")\n",
    "            for root, dirs, files in os.walk(model_snapshot_path):\n",
    "                for file in files:\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    with open(full_path, 'rb') as data:\n",
    "                        key = f\"{s3_model_prefix}/{full_path[len(model_snapshot_path)+1:]}\"\n",
    "                        s3_client.upload_fileobj(data, s3_bucket, key)\n",
    "\n",
    "            print(\"Snapshot uploaded to S3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Steps\n",
    "\n",
    "Next, we process the steps in the pipeline spec. Each step in the pipeline spec has its `config` defined in a YAML file. We use each step's `config` file to generate a step script, which is launched at run time by the [launch.py](./launch.py) script. For each step, we define a [Sagemaker Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) step, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from script_builder import build_script\n",
    "import shutil\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "pipeline_name = pipeline_spec['pipeline']['name']\n",
    "pipeline_version = pipeline_spec['pipeline']['version']\n",
    "release_name=f\"{pipeline_name}-{pipeline_version}\"\n",
    "\n",
    "pipeline_dir = os.path.dirname(os.path.abspath(pipeline_file))\n",
    "pipeline_step_specs = pipeline_spec['pipeline']['steps']\n",
    "\n",
    "s3_output_path = f\"s3://{s3_bucket}/{s3_prefix}/{release_name}\"\n",
    "environment = {\"RELEASE_NAME\": release_name}\n",
    "if hf_model_id:\n",
    "    environment[\"HF_MODEL_ID\"] = hf_model_id\n",
    "if hf_model_revision:\n",
    "    environment[\"HF_MODEL_REVISION\"] = hf_model_revision\n",
    "\n",
    "shutil.copy(\"launch.py\", pipeline_dir)\n",
    "\n",
    "template_file = \"train_script.j2\"\n",
    "\n",
    "pipeline_steps = []\n",
    "\n",
    "for pipeline_step_spec in pipeline_step_specs:\n",
    "    \n",
    "    pipeline_step_config = pipeline_step_spec['config']\n",
    "    print(f\"Processing {pipeline_step_config}\")\n",
    "\n",
    "    # safe load yaml config file\n",
    "    with open(os.path.join(pipeline_dir, pipeline_step_config), \"r\") as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "\n",
    "        pipeline_step_script = pipeline_step_config.replace(\".yaml\", \".sh\")\n",
    "        build_script(template_path=\"templates\", template_file=template_file, \n",
    "            output_file=os.path.join(pipeline_dir, pipeline_step_script),\n",
    "            **config)\n",
    "        \n",
    "        train_config = config.get('train')\n",
    "        distribution = train_config.get('distribution', None)\n",
    "        hyperparameters = {\"script_file\": pipeline_step_script, \n",
    "                           \"torch_distributed\": \"true\" if distribution == \"torch\" else \"false\"}\n",
    "        max_run = train_config.get('max_run', 360000)\n",
    "        resources = config.get('resources', {})\n",
    "        step_estimator = Estimator(\n",
    "            image_uri=ecr_image_uri,\n",
    "            role=role,\n",
    "            entry_point=\"launch.py\",\n",
    "            source_dir=pipeline_dir,\n",
    "            instance_count=resources.get('instance_count', 1),\n",
    "            instance_type=resources.get('instance_type', \"ml.m5.4xlarge\"), \n",
    "            volume_size=resources.get('volume_size', 200),\n",
    "            hyperparameters=hyperparameters,\n",
    "            environment=environment,\n",
    "            max_run=max_run,\n",
    "            distribution = distribution if isinstance(distribution, dict) else None,\n",
    "            subnets=subnets,\n",
    "            security_group_ids=security_group_ids,\n",
    "            output_path=s3_output_path\n",
    "        )\n",
    "\n",
    "        training_step = TrainingStep(\n",
    "            name=pipeline_step_spec['name'],\n",
    "            description=pipeline_step_spec['description'],\n",
    "            estimator=step_estimator,\n",
    "            inputs=data_channels,\n",
    "            depends_on=pipeline_steps[-1:].copy(),\n",
    "        )\n",
    "        pipeline_steps.append(training_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Pipeline Definition\n",
    "\n",
    "Now that we have created all the pipeline steps, we are ready to create the SageMaker Pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=release_name,\n",
    "    steps=pipeline_steps\n",
    ")\n",
    "pipeline.upsert(role_arn=role)\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "print(json.dumps(definition, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Pipeline Execution\n",
    "\n",
    "Having defined the pipeline, now we can start a pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Pipeline Execution\n",
    "\n",
    "Next, we describe pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Pipeline Execution Steps\n",
    "\n",
    "Next, we list the pipeline execution steps. You can run this cell at anytime after starting pipeline execution to get the latest status of pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This concludes the notebook. If you wish, you may create and try your own examples.\n",
    "\n",
    "When you delete the CloudFormation stack, all resources created by the stack, except for the EFS file-system, are deleted. You can reuse the EFS file-system, next time you create a new stack, and the previously stored content on the EFS file-system will be available in your new stack. \n",
    "\n",
    "The content stored on the FSx for Lustre file-system is automatically exported to the S3 bucket. Please verify your content from FSx for Lustre file-system has been successfully exported to your S3 bucket, before you delete the stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

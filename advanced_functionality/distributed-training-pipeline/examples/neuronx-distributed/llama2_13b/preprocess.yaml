resources:
  instance_count: 1
  instance_type: ml.m5.12xlarge 
  volume_size: 200
git:
  repo_url: "https://github.com/aws-neuron/neuronx-distributed.git"
  commit: 3aa65c661c45f133a99052746ed70a0121d0e695
  branch: main
pre_script: 
  - pip3 install --upgrade pip
  - DATA_PATH="$DATA_ROOT/examples_datasets/wikicorpus_llama2_tokenized_4k"
  - 'if [ -d $DATA_PATH ] && [ ! -z "$(ls -A $DATA_PATH)" ]; then echo "$DATA_PATH exists and is not empty" && exit 0; fi'
  - mkdir -p $DATA_ROOT
  - SCRIPT_DIR=$GIT_CLONE_DIR/examples/training/llama/tp_pp_llama_hf_pretrain
  - cp $GIT_CLONE_DIR/examples/training/llama/requirements.txt $SCRIPT_DIR
  - cp $GIT_CLONE_DIR/examples/training/llama/get_dataset.py $SCRIPT_DIR
  - cp $GIT_CLONE_DIR/examples/training/llama/modeling_llama_nxd.py $SCRIPT_DIR
  - cp $SCRIPT_DIR/13B_config_llama2/config.json $SCRIPT_DIR
  - cp $TOKENIZER_MODEL $SCRIPT_DIR
  - cd $SCRIPT_DIR
  - pip3 install -r requirements.txt
train:
  env:
    - name: HOME
      value: $SM_OUTPUT_DATA_DIR
    - name: XDG_CACHE_HOME
      value: $SM_OUTPUT_DATA_DIR
    - name: DATA_ROOT
      value: $SM_CHANNEL_FSX/home/$RELEASE_NAME
    - name: TOKENIZER_MODEL
      value: $SM_CHANNEL_FSX/huggingface/models/$HF_MODEL_ID/$HF_MODEL_REVISION/tokenizer.model
  command:
    -  HOME=$DATA_ROOT python3 
  args:
    - get_dataset.py 
    - '--llama-version 2' 

resources:
  instance_count: 1
  instance_type: ml.g5.48xlarge
  volume_size: 400
pre_script: 
  - export DISTRIBUTED_ARGS="--nproc_per_node $PET_NPROC_PER_NODE --nnodes $PET_NNODES --node_rank $PET_NODE_RANK --master_addr $PET_MASTER_ADDR --master_port $PET_MASTER_PORT"
  - echo "DISTRIBUTED_ARGS=$DISTRIBUTED_ARGS"
  - export LOGS_DIR=$LOG_ROOT/$PET_NODE_RANK
  - mkdir -p $LOGS_DIR
  - export OUTPUT_LOG=$LOGS_DIR/peft_eval.log
  - MODEL="$MODEL_PATH/ckpt.nemo"
  - TEST_DS="[$DATA_ROOT/pubmedqa_test.jsonl]"
  - TEST_NAMES="[pubmedqa]"
  - SCHEME="lora"
  - CONCAT_SAMPLING_PROBS="[1.0]"
  - TP_SIZE=8
  - PP_SIZE=1
  - TOKENS_TO_GENERATE=20
  - OUTPUT_PREFIX=$LOG_ROOT/nemo_experiments/$EXP_NAME/eval_results
  - PATH_TO_TRAINED_MODEL=$LOG_ROOT/nemo_experiments/$EXP_NAME/checkpoints/$EXP_NAME.nemo
  - export HYDRA_FULL_ERROR=1
  - export PEFT_ARGS="
    trainer.devices=$PET_NPROC_PER_NODE
    trainer.num_nodes=$PET_NNODES 
    trainer.precision=bf16
    model.restore_from_path=${MODEL} 
    model.micro_batch_size=24
    model.global_batch_size=96
    ++model.mcore_gpt=True
    model.peft.restore_from_path=${PATH_TO_TRAINED_MODEL} 
    trainer.devices=${PET_NPROC_PER_NODE} 
    model.data.test_ds.file_names=${TEST_DS} 
    model.data.test_ds.names=${TEST_NAMES} 
    model.data.test_ds.global_batch_size=96 
    model.data.test_ds.micro_batch_size=24 
    model.data.test_ds.tokens_to_generate=${TOKENS_TO_GENERATE} 
    model.tensor_model_parallel_size=${TP_SIZE} 
    model.megatron_amp_O2=True 
    model.pipeline_model_parallel_size=${PP_SIZE} 
    inference.greedy=True 
    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} 
    model.answer_only_loss=True 
    model.data.test_ds.write_predictions_to_file=True"
  - SCRIPT_DIR=/NeMo/examples/nlp/language_modeling/tuning
  - cd $HOME
train:
  distribution: "torch"
  env:
    - name: HOME
      value: $SM_OUTPUT_DATA_DIR
    - name: TMPDIR
      value: $SM_OUTPUT_DATA_DIR
    - name: TMP
      value: $SM_OUTPUT_DATA_DIR
    - name: MODEL_PATH
      value: "$SM_CHANNEL_FSX/huggingface/models/$HF_MODEL_ID/$HF_MODEL_REVISION"
    - name: LOG_ROOT
      value: "$SM_CHANNEL_EFS/home/$RELEASE_NAME/logs"
    - name: DATA_ROOT
      value: "$SM_CHANNEL_FSX/home/$RELEASE_NAME/data"
    - name: EXP_NAME
      value: "peft_pubmedqa"
    - name: NCCL_DEBUG
      value: "WARN"
    - name: NCCL_SOCKET_IFNAME 
      value: "eth0"
  command:
    -  "torchrun"
  args: 
    - $DISTRIBUTED_ARGS
    - $SCRIPT_DIR/megatron_gpt_generate.py
    - $PEFT_ARGS
    - '2>&1 | tee $OUTPUT_LOG' 

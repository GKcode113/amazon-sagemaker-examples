{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model (LLM) Inference Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "This notebook is for real-time Large Language Model (LLM) testing in SageMaker. This notebook supports [Deep Java Library (djl) Large Model Inference (LMI)](https://docs.djl.ai/docs/serving/serving/docs/lmi/conceptual_guide/lmi_engine.html) and [Triton Inference Server](https://developer.nvidia.com/triton-inference-server) to deploy the models in SageMaker.\n",
    "\n",
    "This notebook is driven by [YAML recipe files](#yaml-recipe-ile). The `test` object in the YAML recipe file defines the testing module interface. The testing module specifies a prompt generator class that yields prompts for the tests. This notebook supports both Unit Testing, and load testing using open-source [Locust](https://locust.io/).  \n",
    "\n",
    "The key purpose of this notebook is to explore the inference engine configuration space to find optimal configuration for a given use case. Example configurations are included under the `examples` folder. You should use this notebook to try different configuration settings, and find the optimal settings for your use case.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook should be run in an [Amazon SageMaker Notebook](https://aws.amazon.com/sagemaker-ai/notebooks/) using `ml.m5.2xlarge`, or larger, instance.\n",
    "\n",
    "Since we are working with LLM inference, we will need to download HuggingFace model snapshots, and upload them to S3 bucket. We may also need to build Docker containers. To be able to do these actions without running out of disk space, configure at least 500 GB (1000 GB recommended) volume with the SageMaker Notebook instance, and configure this volume to be used as Docker overlay file-system by executing following steps:\n",
    "\n",
    "#### Create new Docker overlay file-system directory:\n",
    "\n",
    "    sudo mkdir -p /home/ec2-user/SageMaker/docker-overlay\n",
    "    sudo chown root:root /home/ec2-user/SageMaker/docker-overlay\n",
    "\n",
    "#### Update the Docker overlay file-system configuration \n",
    "\n",
    "Add following content to `/etc/docker/daemon.json`\n",
    "\n",
    "    {\n",
    "      \"data-root\": \"/home/ec2-user/SageMaker/docker-overlay\",\n",
    "      \"runtimes\": {\n",
    "          \"nvidia\": {\n",
    "              \"path\": \"nvidia-container-runtime\",\n",
    "              \"runtimeArgs\": []\n",
    "          }\n",
    "      }\n",
    "    }\n",
    "\n",
    "#### Restart Docker daemon\n",
    "\n",
    "    sudo systemctl stop docker\n",
    "    sudo systemctl start docker\n",
    "    sudo systemctl status docker.service\n",
    "\n",
    "\n",
    "## YAML Recipe File\n",
    "\n",
    "To deploy a model, you need to define a [YAML recipe file](./examples/vllm/meta-llama3-8b-instruct/config.yaml). Below, we provide a brief explanation for the syntax of the YAML recipe:\n",
    "\n",
    "* The `huggingface` object is optional: \n",
    "    * If you specify `huggingface` object, the `huggingface.name` field is required\n",
    "    * The `huggingface.revision` is required if `huggingface.download` is `true`.\n",
    "    * if `huggingface` is specified, `djl` object must be specified\n",
    "* The `djl` object is optional and if specified contains the content for [DJL serving.properties](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html) for the selected [LMI engine](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/index.html)\n",
    "    * If you specify `huggingface` object, `djl.option\\.model_id` is computed automatically.\n",
    "* The `sagemaker.model` object is required \n",
    "    * The field `sagemaker.model.image` is optional: Alternatively, you can specify `sagemaker.model.container`  as relative path (w.r.t. to this notebook) to the container build script directory.\n",
    "    * The field `sagemaker.model.env` is optional\n",
    "* The `sagemaker.endpoint` object is required\n",
    "    * In `sagemaker.endpoint` object, only `name` and `instance_type` are required.\n",
    "* The `test` object is optional, and defines the test interface \n",
    "    * The `test.module_dir` is relative path. This path is added to `sys.path`. If there is a `requirements.txt` in this directory, it is installed.\n",
    "    * The `test.module_name` must be a Python module in `test.module_dir`. This module is dynamically loaded.\n",
    "    * The `test.prompt_generator` is the name of a class in the `test.module_name` module. An object of this class is dyamically created, and the `__call__` method on the object is called to get the prompt generator for testing.\n",
    "    * The `test.warmpup_iters` are used to warmup the deployed inference model.\n",
    "    * The `n_concurrent` sepecifies number of concurrent processes used in Unit Testing.\n",
    "    * The `test.max_iters` limits the number of prompt requests, not including the `test.warmup_iters`.\n",
    "    * The `test.output_dir` is the relative path to the testing output root directory. \n",
    "    * The `test.locust_users` and `test.locust_workers` fields specify Locust Users and Workers, respectively, used in Locust Testing.\n",
    "    * The `test.template` and `test.template_keys` fields specify the structure of the prompt template. The keys are used to update the template with prompt values.\n",
    "    \n",
    "### Custom Model Handler Code\n",
    "\n",
    "You can optionally add a [custom model handler](https://github.com/deepjavalibrary/djl-serving/blob/bc7fdfdcbb66b982522e6bc809b0044fabde69e0/serving/docs/streaming_config.md#custom-modelpy-handler) in the `code` sub-folder, collocated with the LMI configuration file, and it is added to the SageMaker model package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Notebook\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "Next, install latest version of required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SageMaker Session\n",
    "\n",
    "Below, we initialize the SageMaker session. If no `s3_bucket` is set below, the default SageMaker bucket in the region is used.Specify below the optional `s3_bucket` and `s3_prefix` that we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "s3_bucket = None  # specify bucket, or use default sagemaker bucket, if it exists\n",
    "s3_prefix = \"lmi-djl\"  # Large model inference with deep java library\n",
    "\n",
    "role = sagemaker.get_execution_role()  # you may provide a pre-existing role ARN here\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "aws_region = session.region_name\n",
    "print(f\"AWS Region: {aws_region}\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "try:\n",
    "    if s3_bucket is None:\n",
    "        s3_bucket = sagemaker_session.default_bucket()\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    response = s3_client.get_bucket_location(Bucket=s3_bucket)\n",
    "    bucket_region = response[\"LocationConstraint\"]\n",
    "    bucket_region = \"us-east-1\" if bucket_region is None else bucket_region\n",
    "\n",
    "    print(f\"Bucket region: {bucket_region}\")\n",
    "\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=s3_bucket, Key=f\"{s3_prefix}/\")\n",
    "    except:\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=f\"{s3_prefix}/\")\n",
    "\n",
    "    print(f\"Using S3 folder: s3://{s3_bucket}/{s3_prefix}/ in this notebook\")\n",
    "except:\n",
    "    print(\n",
    "        f\"Access Error: Check if '{s3_bucket}' S3 bucket is in '{aws_region}' region, and {s3_prefix} path exists\"\n",
    "    )\n",
    "\n",
    "sts = boto3.client(\"sts\")\n",
    "aws_account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(f\"AWS Account Id: {aws_account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify YAML Recipe\n",
    "\n",
    "We specify a YAML file for the model we wish to deploy and test. For example, we specify [vLLM LMI configuration file for Llama3 8B below](./examples/vllm/meta-llama3-8b-instruct/config.yaml), below. You may change `config_path` below to the *relative path* of `examples` YAML file you wish to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "import pathlib\n",
    "from utils import install_pip_requirements, install_pip_package\n",
    "\n",
    "print(f\"Current working directory: {pathlib.Path().resolve()}\")\n",
    "\n",
    "config_path = \"examples/vllm/meta-llama3-8b-instruct/config.yaml\"\n",
    "with open(config_path, \"r\") as mf:\n",
    "    model_config = yaml.safe_load(mf)\n",
    "\n",
    "print(\"\\nmodel_config:\\n\")\n",
    "print(json.dumps(model_config, indent=2))\n",
    "\n",
    "assert (\n",
    "    model_config.get(\"huggingface\", None) is None or model_config.get(\"djl\", None) is not None\n",
    "), \"'djl' must be specified if 'huggingface' is specified\"\n",
    "\n",
    "assert model_config.get(\"sagemaker\", None), \"'sagemaker' object is required\"\n",
    "assert model_config[\"sagemaker\"].get(\"model\", None), \"'sagemaker.model' is required\"\n",
    "assert model_config[\"sagemaker\"].get(\"endpoint\", None), \"'sagemaker.endpoint' is required\"\n",
    "\n",
    "test_spec = model_config.get(\"test\", None)\n",
    "if test_spec:\n",
    "    requirements_path = os.path.join(os.path.dirname(config_path), \"requirements.txt\")\n",
    "    if os.path.isfile(requirements_path):\n",
    "        install_pip_requirements(requirements_path)\n",
    "    else:\n",
    "        install_pip_package(package_name=\"transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe Build and Push Model Inference Container Image to ECR\n",
    "\n",
    "Next, if `sagemaker.model.container` is specified, we build and push the container image to Amazon ECR. After building and pushing the image to ECR, we update the `sagemaker.model.image` with the ECR URI for the image. Either `container` or `image` must be specified in `sagemaker.model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, stat, re\n",
    "from utils import push_ecr_container\n",
    "\n",
    "sm_model_config = model_config[\"sagemaker\"][\"model\"]\n",
    "container_path = sm_model_config.get(\"container\", None)\n",
    "\n",
    "if container_path is not None:\n",
    "    sm_model_config[\"image\"] = push_ecr_container(\n",
    "        container_path=container_path, aws_region=aws_region, aws_account_id=aws_account_id\n",
    "    )\n",
    "else:\n",
    "    ecr_image_uri = sm_model_config.get(\"image\", None)\n",
    "    assert (\n",
    "        ecr_image_uri is not None\n",
    "    ), \"'sagemaker.model.image' or 'sagemaker.model.container' is required\"\n",
    "    pattern = \"\\.dkr\\.ecr\\.[a-z0-9-]+\\.\"\n",
    "    replace = f\".dkr.ecr.{aws_region}.\"\n",
    "    sm_model_config[\"image\"] = re.sub(pattern, replace, ecr_image_uri)\n",
    "\n",
    "print(f\"Model serving image: {sm_model_config['image']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe Download HuggingFace Model Snapshot\n",
    "\n",
    "If `huggingface` object is specified, only `huggingface.model` is required. If `huggingface.download` is `true`, the HuggingFace model snapshot with revision `huggingface.revision` is downloaded and uploaded to the `s3_bucket`, and `djl.option\\.model_id` is set to the S3 URI of the uploaded HuggingFace model snapshot. \n",
    "\n",
    "**Note: You must specify `hf_token` below if the Hugging Face model requires a Hugging Face token for downloading the model from the HuggingFace hub. This is true whether or not you specify the `huggingface` object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import snapshot_hf_model_to_s3\n",
    "\n",
    "hf_token = None  # Specify HuggingFace token, if required to access model\n",
    "hf_spec = model_config.get(\"huggingface\", None)\n",
    "\n",
    "if hf_spec:\n",
    "    snapshot_hf_model_to_s3(\n",
    "        s3_client,\n",
    "        s3_prefix=s3_prefix,\n",
    "        s3_bucket=s3_bucket,\n",
    "        model_config=model_config,\n",
    "        hf_spec=hf_spec,\n",
    "        hf_token=hf_token,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SageMaker Model Package\n",
    "\n",
    "Next, we create the model package TAR ball, and upload it to `s3_bucket`. The model package will be used to define the SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import djl_model_package_to_s3\n",
    "from utils import triton_model_package_to_s3\n",
    "\n",
    "sm_model_name = sm_model_config.get(\"name\", None)\n",
    "assert sm_model_name, \"'sagemaker.model.name' is required\"\n",
    "djl_spec = model_config.get(\"djl\", None)\n",
    "tritonserver = sm_model_config.get(\"tritonserver\", None)\n",
    "\n",
    "model_pkg_key = None\n",
    "if djl_spec is not None:\n",
    "    model_pkg_key = djl_model_package_to_s3(\n",
    "        s3_client,\n",
    "        djl_spec=djl_spec,\n",
    "        sm_model_name=sm_model_name,\n",
    "        config_path=config_path,\n",
    "        s3_prefix=s3_prefix,\n",
    "        s3_bucket=s3_bucket,\n",
    "    )\n",
    "elif tritonserver is not None:\n",
    "    model_pkg_key = triton_model_package_to_s3(\n",
    "        s3_client,\n",
    "        sm_model_name=sm_model_name,\n",
    "        config_path=config_path,\n",
    "        s3_prefix=s3_prefix,\n",
    "        s3_bucket=s3_bucket,\n",
    "    )\n",
    "\n",
    "assert model_pkg_key, \"Model package key is not set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Next, we create the SageMaker model using the model package we just uploaded tp `s3_bucket`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "sm_model_image = sm_model_config.get(\"image\", None)\n",
    "assert sm_model_image, \"'sagemaker.model.image' is required\"\n",
    "\n",
    "sm_model_env = sm_model_config.get(\"env\", {})\n",
    "if (tritonserver or sm_model_env.get(\"HF_MODEL_ID\", None) is not None) and hf_token is not None:\n",
    "    sm_model_env[\"HF_TOKEN\"] = hf_token\n",
    "\n",
    "primary_container = {\"Image\": sm_model_image, \"Environment\": sm_model_env}\n",
    "if model_pkg_key is not None:\n",
    "    primary_container[\"ModelDataUrl\"] = f\"s3://{s3_bucket}/{model_pkg_key}\"\n",
    "\n",
    "try:\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName=sm_model_name,\n",
    "        ExecutionRoleArn=role,\n",
    "        PrimaryContainer=primary_container,\n",
    "    )\n",
    "    model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "    print(f\"Created Model: {model_arn}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint Config\n",
    "\n",
    "Next we create endpoint config for the SageMaker model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_endpoint_spec = model_config[\"sagemaker\"][\"endpoint\"]\n",
    "endpoint_name = sm_endpoint_spec.get(\"name\", None)\n",
    "assert endpoint_name, \"'sagemaker.endpoint.name' is required\"\n",
    "\n",
    "variant_name = sm_endpoint_spec.get(\"variant_name\", \"test\")\n",
    "\n",
    "instance_type = sm_endpoint_spec.get(\"instance_type\", None)\n",
    "assert instance_type, \"'sagemaker.endpoint.instance_type' is required\"\n",
    "\n",
    "initial_instance_count = sm_endpoint_spec.get(\"initial_instance_count\", 1)\n",
    "model_data_download_timeout_secs = sm_endpoint_spec.get(\"model_data_download_timeout_secs\", 1200)\n",
    "container_startup_health_check_timeout_secs = sm_endpoint_spec.get(\n",
    "    \"container_startup_health_check_timeout_secs\", 1200\n",
    ")\n",
    "\n",
    "production_variant = {\n",
    "    \"VariantName\": variant_name,\n",
    "    \"ModelName\": sm_model_name,\n",
    "    \"InstanceType\": instance_type,\n",
    "    \"InitialInstanceCount\": initial_instance_count,\n",
    "    \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_secs,\n",
    "    \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_secs,\n",
    "    \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "}\n",
    "\n",
    "volume_size_gb = sm_endpoint_spec.get(\"volume_size_gb\", None)\n",
    "if volume_size_gb:\n",
    "    production_variant[\"VolumeSizeInGB\"] = volume_size_gb\n",
    "\n",
    "try:\n",
    "    endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_name, ProductionVariants=[production_variant]\n",
    "    )\n",
    "    print(endpoint_config_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating endpoint config: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint\n",
    "\n",
    "Next, we create the SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "try:\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, EndpointConfigName=endpoint_name\n",
    "    )\n",
    "    print_counter = 0\n",
    "    while True:\n",
    "        response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = response[\"EndpointStatus\"]\n",
    "        if status == \"InService\":\n",
    "            print(f\"Endpoint {endpoint_name} is ready!\")\n",
    "            break\n",
    "        elif status == \"Failed\":\n",
    "            print(f\"Endpoint creation failed: {response['FailureReason']}\")\n",
    "            break\n",
    "        if print_counter % 10 == 0:\n",
    "            print(f\"Endpoint status: {status}...\")\n",
    "        print_counter += 1\n",
    "        time.sleep(30)  # Wait for 30 seconds before checking again\n",
    "except Exception as e:\n",
    "    print(f\"Error creating endpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Unit Testing\n",
    "\n",
    "Now, we are ready to run our unit tests, using the test interface. During this step, we download model tokenizer configuration files, and create a tokenizer. Next, we dynamically load the test interface Python module, create a prompt generator class object, and use it to drive our unit test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_spec:\n",
    "    from test_task import test_task\n",
    "\n",
    "    prompt_module_dir = test_spec.get(\"module_dir\", None)\n",
    "    assert prompt_module_dir, \"'test.module_dir' is required\"\n",
    "    requirements_path = os.path.join(prompt_module_dir, \"requirements.txt\")\n",
    "    if os.path.isfile(requirements_path):\n",
    "        install_pip_requirements(requirements_path)\n",
    "\n",
    "    sm_endpoint_spec = model_config[\"sagemaker\"][\"endpoint\"]\n",
    "    endpoint_name = sm_endpoint_spec.get(\"name\", None)\n",
    "    assert endpoint_name, \"'sagemaker.endpoint.name' is required\"\n",
    "\n",
    "    instance_type = sm_endpoint_spec.get(\"instance_type\", None)\n",
    "    assert instance_type, \"'sagemaker.endpoint.instance_type' is required\"\n",
    "\n",
    "    seq_len_options = [\n",
    "        \"option.max_tokens\",\n",
    "        \"option.n_positions\",\n",
    "        \"option.max_model_len\",\n",
    "        \"option.max_num_tokens\",\n",
    "    ]\n",
    "    seq_len = 0\n",
    "\n",
    "    if djl_spec is not None:\n",
    "        for seq_len_option in seq_len_options:\n",
    "            seq_len = djl_spec.get(seq_len_option, 0)\n",
    "            if seq_len:\n",
    "                break\n",
    "\n",
    "    tp_degree = djl_spec.get(\"option.tensor_parallel_degree\", 1) if djl_spec is not None else 1\n",
    "    rbs = djl_spec.get(\"option.max_rolling_batch_size\", 1) if djl_spec is not None else 1\n",
    "\n",
    "    instance_count = sm_endpoint_spec.get(\"initial_instance_count\", 1)\n",
    "\n",
    "    output_dir = test_spec.get(\"output_dir\", None)\n",
    "    assert output_dir, \"'test.output_dir' is required\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results_path = os.path.join(\n",
    "        output_dir,\n",
    "        \"unit-testing\",\n",
    "        sm_model_name,\n",
    "        f\"type={instance_type}\",\n",
    "        f\"count={instance_count}\",\n",
    "        f\"slen={seq_len}\",\n",
    "        f\"tp={tp_degree}\",\n",
    "        f\"rbs={rbs}\",\n",
    "    )\n",
    "\n",
    "    print(f\"Unit results path: {results_path}\")\n",
    "\n",
    "    output_formatter = djl_spec.get(\"option.output_formatter\", None) if djl_spec else None\n",
    "    rolling_batch = djl_spec.get(\"option.rolling_batch\", None) if djl_spec else None\n",
    "    streaming_enabled = rolling_batch in [\n",
    "        \"auto\",\n",
    "        \"deepspeed\",\n",
    "        \"trtllm\",\n",
    "        \"vllm\",\n",
    "    ] and output_formatter in [\"jsonlines\"]\n",
    "    print(f\"Streaming enabled: {streaming_enabled}\")\n",
    "\n",
    "    model_id = (\n",
    "        djl_spec.get(\"option.model_id\")\n",
    "        if djl_spec\n",
    "        else sm_model_env.get(\"MODEL_ID\", sm_model_env.get(\"HF_MODEL_ID\"))\n",
    "    )\n",
    "    test_task(\n",
    "        model_id=model_id,\n",
    "        test_spec=test_spec,\n",
    "        endpoint_name=endpoint_name,\n",
    "        results_path=results_path,\n",
    "        streaming_enabled=streaming_enabled,\n",
    "        hf_token=hf_token,\n",
    "    )\n",
    "\n",
    "    print(f\"Uploading unit results to S3 bucket...\")\n",
    "    for root, dirs, files in os.walk(results_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            with open(full_path, \"rb\") as data:\n",
    "                s3_client.upload_fileobj(data, s3_bucket, full_path)\n",
    "    print(f\"Uploading unit results to S3 bucket completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Unit Test Results\n",
    "\n",
    "Below, we visualize the unit test results to make sure the model output is as expected. This allows us to verify that our endpoint was deployed correctly, and that there are no configuration errors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "if test_spec:\n",
    "    results_files = glob.glob(os.path.join(results_path, \"*.json\"))\n",
    "    result_latest = max(results_files, key=os.path.getmtime)\n",
    "\n",
    "    result_latest_parts = result_latest.split(\"/\")[1:-2]\n",
    "    caption = \" \".join(result_latest_parts).upper()\n",
    "\n",
    "    df = pd.read_json(result_latest, lines=True)\n",
    "\n",
    "    top_n = 5\n",
    "    df = df.truncate(after=top_n - 1, axis=0)\n",
    "    df.index += 1\n",
    "    df = (\n",
    "        df.style.format(precision=5)\n",
    "        .format_index(str.upper, axis=1)\n",
    "        .set_properties(**{\"text-align\": \"left\"})\n",
    "        .set_caption(caption)\n",
    "    )\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install locust\n",
    "!which locust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locust Testing\n",
    "\n",
    "If the configuration file includes `test` object, we can do [Locust](https://locust.io/) based throughput testing, as shown below. Next cell will run until testing is complete. The duration of the testing is specified in  `os.environ[\"RUN_TIME\"]` below, which by default is 2 minutes. The `os.environ[\"SPAWN_RATE\"]` specifies ramp up rate for the Locust Users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "ts = round(time.time() * 1000)\n",
    "if test_spec:\n",
    "    os.environ[\"STREAMING_ENABLED\"] = str(streaming_enabled)\n",
    "    os.environ[\"CONTENT_TYPE\"] = \"application/json\"\n",
    "    os.environ[\"ENDPOINT_NAME\"] = (\n",
    "        f\"https://runtime.sagemaker.{aws_region}.amazonaws.com/endpoints/{endpoint_name}/invocations\"\n",
    "    )\n",
    "    os.environ[\"USERS\"] = str(test_spec.get(\"locust_users\", 4))\n",
    "    os.environ[\"WORKERS\"] = str(test_spec.get(\"locust_workers\", 1))\n",
    "    os.environ[\"RUN_TIME\"] = \"2m\"\n",
    "    os.environ[\"SPAWN_RATE\"] = str(test_spec.get(\"locust_users\", 4))\n",
    "    os.environ[\"SCRIPT\"] = \"endpoint_user.py\"\n",
    "    results_locust_path = os.path.join(\n",
    "        output_dir,\n",
    "        \"locust-testing\",\n",
    "        sm_model_name,\n",
    "        f\"type={instance_type}\",\n",
    "        f\"count={instance_count}\",\n",
    "        f\"slen={seq_len}\",\n",
    "        f\"tp={tp_degree}\",\n",
    "        f\"rbs={rbs}\",\n",
    "    )\n",
    "    os.environ[\"RESULTS_PREFIX\"] = f\"{results_locust_path}/results-{ts}\"\n",
    "    os.environ[\"TASK_NAME\"] = test_spec.get(\"task_name\", \"text-generation\")\n",
    "\n",
    "    try:\n",
    "        with open(\"run_locust.log\", \"w\") as logfile:\n",
    "            print(f\"Start Locust testing; logfile: run_locust.log; results: {results_locust_path}\")\n",
    "            path = os.path.join(os.getcwd(), \"run_locust.sh\")\n",
    "            os.chmod(path, stat.S_IRUSR | stat.S_IEXEC)\n",
    "            process = subprocess.Popen(\n",
    "                path, encoding=\"utf-8\", shell=True, stdout=logfile, stderr=subprocess.STDOUT\n",
    "            )\n",
    "            process.wait()\n",
    "            logfile.flush()\n",
    "            print(f\"Locust testing completed\")\n",
    "\n",
    "        print(f\"Uploading locust results to S3 bucket...\")\n",
    "        for root, dirs, files in os.walk(results_locust_path):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                with open(full_path, \"rb\") as data:\n",
    "                    s3_client.upload_fileobj(data, s3_bucket, full_path)\n",
    "        print(f\"Uploading locust results to S3 bucket completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"exception occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Locust Results\n",
    "\n",
    "Below we first visualize the results of the Locust testing in a tabel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "if test_spec:\n",
    "    results_path = os.environ[\"RESULTS_PREFIX\"] + \"_stats.csv\"\n",
    "    df = pd.read_csv(results_path)\n",
    "    df = df.replace(np.nan, \"\")\n",
    "\n",
    "    top_n = 1\n",
    "    caption = f\"{endpoint_name} type={instance_type} count={instance_count} tp={tp_degree} slen={seq_len} rbs={rbs}\".upper()\n",
    "    df = df.truncate(after=top_n - 1, axis=0)\n",
    "    df = df.style.format(precision=6).set_properties(**{\"text-align\": \"left\"}).set_caption(caption)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the key metrics in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if test_spec:\n",
    "    results_path = os.environ[\"RESULTS_PREFIX\"] + \"_stats.csv\"\n",
    "    df = pd.read_csv(results_path)\n",
    "    df = df[\n",
    "        [\n",
    "            \"Median Response Time\",\n",
    "            \"Average Response Time\",\n",
    "            \"Min Response Time\",\n",
    "            \"Max Response Time\",\n",
    "            \"95%\",\n",
    "        ]\n",
    "    ]\n",
    "    df = df.replace(np.nan, \"\")\n",
    "\n",
    "    top_n = 1\n",
    "    df = df.truncate(after=top_n - 1, axis=0)\n",
    "    df.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "This concludes the notebook. Below. we delete the **deployed** SageMaker endpoint, endpoint configuration, and the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Delete Endpoint response: {response}\")\n",
    "\n",
    "response = sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "print(f\"Delete Endpoint Config response: {response}\")\n",
    "\n",
    "response = sm_client.delete_model(ModelName=sm_model_name)\n",
    "print(f\"Delete Model response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
